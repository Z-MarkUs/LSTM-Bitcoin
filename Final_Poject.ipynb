{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# APS1052 Final Project - LSTM Bitcoin",
   "id": "62184b3f4ea91dc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "938385b6261d3fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data\n",
    "The data set is already downloaded for us from the Glassnode"
   ],
   "id": "44d4db208878e7bf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load the dataset\n",
    "file_path = \"data/btc_dataset.csv\"  # Replace with your dataset path\n",
    "df = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Preview the data\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if there is any missing value.",
   "id": "5171531e067bbf5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "# Display the result\n",
    "if missing_columns.empty:\n",
    "    print(\"No missing values in the dataset.\")\n",
    "else:\n",
    "    print(\"Missing values in each column:\")\n",
    "    print(missing_columns)\n",
    "\n",
    "    # Visualize missing data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_columns.plot(kind='bar', title=\"Count of Missing Values by Column\")\n",
    "    plt.xlabel(\"Columns\")\n",
    "    plt.ylabel(\"Number of Missing Values\")\n",
    "    plt.show()\n"
   ],
   "id": "859b626ed3baba3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Core",
   "id": "981eaa79075a68f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Data",
   "id": "6de0416472bfac2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature Engineering",
   "id": "3bca780991e94263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. Realized Volatility (14-day Rolling)\n",
    "- **How to Derive**: Compute the rolling standard deviation of the \"Closing Price (USD)\" log returns over the past 14 days.\n",
    "- **Implementation**:\n",
    "    $$\n",
    "    \\text{Realized Volatility} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (\\log(P_t/P_{t-1}))^2}\n",
    "    $$\n",
    "- **Relevant Columns**: `\"Closing Price (USD)\"`"
   ],
   "id": "33fb383aee3f3460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate log returns\n",
    "df['log_return'] = np.log(df['Closing Price (USD)'] / df['Closing Price (USD)'].shift(1))\n",
    "\n",
    "# Calculate realized volatility (14-day rolling standard deviation)\n",
    "df['realized_volatility'] = df['log_return'].rolling(window=14).std()\n",
    "\n",
    "# Drop the temporary column\n",
    "df.drop(columns=['log_return'], inplace=True)\n",
    "\n",
    "# Visualize the feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['realized_volatility'].plot(title=\"Realized Volatility (14-day Rolling)\")\n",
    "plt.show()\n"
   ],
   "id": "e580026297b8cb52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate log returns\n",
    "df['log_return'] = np.log(df['Closing Price (USD)'] / df['Closing Price (USD)'].shift(1))\n",
    "\n",
    "# Calculate realized volatility (14-day rolling standard deviation)\n",
    "df['realized_volatility'] = df['log_return'].rolling(window=14).std()\n",
    "\n",
    "# Drop the temporary column\n",
    "df.drop(columns=['log_return'], inplace=True)\n",
    "\n",
    "# Visualize the feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['realized_volatility'].plot(title=\"Realized Volatility (14-day Rolling)\")\n",
    "plt.show()\n"
   ],
   "id": "c21dc04ac609a90f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. Exchange Netflow Proxy\n",
    "- **How to Derive**: Use `\"active_addresses\"` as a proxy for activity. Calculate the daily change in `\"active_addresses\"` or the ratio of `\"active_addresses\"` to `\"total_addresses\"`. This can act as a proxy for inflow/outflow activity.\n",
    "- **Implementation**:\n",
    "$$\n",
    "     \\text{Netflow Proxy} = \\frac{\\text{active\\_addresses}}{\\text{total\\_addresses}}\n",
    "     $$\n",
    "- **Relevant Columns**: `\"active_addresses\"`, `\"total_addresses\"`\n"
   ],
   "id": "c772d98ce8add2ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ratio of active addresses to total addresses as a proxy for netflow\n",
    "df['exchange_netflow_proxy'] = df['active_addresses'] / df['total_addresses']\n",
    "\n",
    "# Visualize the feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['exchange_netflow_proxy'].plot(title=\"Exchange Netflow Proxy\")\n",
    "plt.show()\n"
   ],
   "id": "b6689aac76f0ebed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. Social Sentiment Proxy (Google Popularity Change)\n",
    "- **How to Derive**: Use the percentage change or rolling average of `\"Google_popularity\"` to capture shifts in public interest over time.\n",
    "- **Implementation**:\n",
    "     $$\n",
    "     \\text{Google Sentiment} = \\frac{\\text{Google\\_popularity}_t - \\text{Google\\_popularity}_{t-1}}{\\text{Google\\_popularity}_{t-1}}\n",
    "     $$\n",
    "- **Relevant Columns**: `\"Google_popularity\"`"
   ],
   "id": "f89248a45df6b01b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Percentage change in Google Popularity\n",
    "df['google_sentiment_proxy'] = df['Google_popularity'].pct_change()\n",
    "\n",
    "# Visualize the feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['google_sentiment_proxy'].plot(title=\"Google Sentiment Proxy\")\n",
    "plt.show()\n"
   ],
   "id": "583d5e6689aec6ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4. Funding Rates Proxy (Mining Cost Ratio)\n",
    "   - **How to Derive**: Calculate the ratio of `\"hash_rate\"` to `\"difficulty\"`, which can act as a proxy for mining costs and potential supply-side pressures.\n",
    "   - **Implementation**:\n",
    "     $$\n",
    "     \\text{Mining Cost Ratio} = \\frac{\\text{hash\\_rate}}{\\text{difficulty}}\n",
    "     $$\n",
    "   - **Relevant Columns**: `\"hash_rate\"`, `\"difficulty\"`\n"
   ],
   "id": "bb9fb6aa548bf90f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert columns to numeric\n",
    "df['hash_rate'] = pd.to_numeric(df['hash_rate'], errors='coerce')\n",
    "df['difficulty'] = pd.to_numeric(df['difficulty'], errors='coerce')\n",
    "\n",
    "# Mining cost ratio as a proxy for funding rates\n",
    "df['funding_rate_proxy'] = df['hash_rate'] / df['difficulty']\n",
    "\n",
    "# Visualize the feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['funding_rate_proxy'].plot(title=\"Funding Rates Proxy\")\n",
    "plt.show()\n"
   ],
   "id": "b8ea18bfb2c1f9f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5. Macro Interaction Proxy (GLD-SPY Interaction)\n",
    "   - **How to Derive**: Create interaction terms between `\"GLD\"` (gold prices) and `\"SPY\"` (S&P 500 index) to capture broader macroeconomic influences. This could be the product or difference between the two.\n",
    "   - **Implementation**:\n",
    "     $$\n",
    "     \\text{GLD-SPY Interaction} = \\text{GLD} \\times \\text{SPY}\n",
    "     $$\n",
    "   - **Relevant Columns**: `\"GLD\"`, `\"SPY\"`"
   ],
   "id": "17d04140c01470d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if there is any missing value.",
   "id": "7e2efc6aa81123e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Interaction between GLD and SPY\n",
    "df['macro_interaction_proxy'] = df['GLD'] * df['SPY']\n",
    "\n",
    "# Visualize the feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['macro_interaction_proxy'].plot(title=\"Macro Interaction Proxy (GLD * SPY)\")\n",
    "plt.show()\n"
   ],
   "id": "28573a982f218bdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "# Display the result\n",
    "if missing_columns.empty:\n",
    "    print(\"No missing values in the dataset.\")\n",
    "else:\n",
    "    print(\"Missing values in each column:\")\n",
    "    print(missing_columns)\n",
    "\n",
    "    # Visualize missing data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_columns.plot(kind='bar', title=\"Count of Missing Values by Column\")\n",
    "    plt.xlabel(\"Columns\")\n",
    "    plt.ylabel(\"Number of Missing Values\")\n",
    "    plt.show()\n"
   ],
   "id": "84869f2a0cc10abc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This caused by the way we calculate the two features.\n",
    "We have two different options:\n",
    "1. `NaN`\n",
    "2. `0`\n",
    "\n",
    "Which Option to Choose?\n",
    "- **For Long-Term Models or Sparse Data:** Use **Option 1** (`NaN`) and drop rows with missing values. This maintains the integrity of the calculations.\n",
    "- **For Time Series Models with Long Sequences:** Use **Option 2** (fill with `0`) to avoid breaking sequence continuity.\n",
    "\n",
    "Since we are working with LSTMs and sequences, Option 2 (setting to `0`) is more practical:\n",
    "- It avoids disrupting sequences, which is critical for time series models.\n",
    "- The missing data period (**14** days for volatility, **1** day for sentiment) is short, so the impact of filling with `0` is minimal.\n"
   ],
   "id": "2b59dc5e9595d197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['realized_volatility'] = df['realized_volatility'].fillna(0)\n",
    "df['google_sentiment_proxy'] = df['google_sentiment_proxy'].fillna(0)"
   ],
   "id": "b67926843081a9ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the current DataFrame to a CSV file\n",
    "file_path = \"data/BTC_data.csv\"\n",
    "df.to_csv(file_path, index=True)"
   ],
   "id": "8a2a572c2529d9ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Preparation",
   "id": "c2a327be44de3942"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dataset Split",
   "id": "29b4d7b7e9011d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define split ratios\n",
    "train_ratio = 0.64  # 64% for training (80% of Train + Validation)\n",
    "validation_ratio = 0.16  # 16% for validation (20% of Train + Validation)\n",
    "test_ratio = 0.20  # 20% for testing\n",
    "\n",
    "# Calculate indices for splits\n",
    "train_end_index = int(len(df) * train_ratio)\n",
    "validation_end_index = int(len(df) * (train_ratio + validation_ratio))\n",
    "\n",
    "# Sequential split\n",
    "train_data = df.iloc[:train_end_index]\n",
    "validation_data = df.iloc[train_end_index:validation_end_index]\n",
    "test_data = df.iloc[validation_end_index:]\n",
    "\n",
    "# Print the shapes\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {validation_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n"
   ],
   "id": "48b5dfef8ef20384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data Normalization",
   "id": "f34f2228f634ae98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to normalize sequences for training (with leading zero handling)\n",
    "def normalize_sequences_train(data, columns_with_leading_zeros):\n",
    "    normalized_data = []\n",
    "    for i in range(len(data) - lookback_window):\n",
    "        sequence = data.iloc[i:i + lookback_window].copy()  # Copy the sequence\n",
    "        base = sequence.iloc[0].copy()  # Explicitly copy the first row as baseline\n",
    "\n",
    "        # For problematic columns, find the first non-zero value\n",
    "        for col in columns_with_leading_zeros:\n",
    "            non_zero_index = sequence[col].ne(0).idxmax()  # Find the index of the first non-zero value\n",
    "            if pd.notnull(non_zero_index):  # Ensure the index exists\n",
    "                base[col] = sequence.loc[non_zero_index, col]  # Set the first non-zero value as the baseline\n",
    "\n",
    "        # Normalize the sequence\n",
    "        normalized_sequence = (sequence / base) - 1\n",
    "        normalized_data.append(normalized_sequence.values)\n",
    "\n",
    "    return np.array(normalized_data)\n",
    "\n",
    "# Function to normalize sequences for validation and test (no leading zero handling needed)\n",
    "def normalize_sequences(data):\n",
    "    normalized_data = []\n",
    "    for i in range(len(data) - lookback_window):\n",
    "        sequence = data.iloc[i:i + lookback_window].copy()\n",
    "        base = sequence.iloc[0].copy()\n",
    "        normalized_sequence = (sequence / base) - 1\n",
    "        normalized_data.append(normalized_sequence.values)\n",
    "\n",
    "    return np.array(normalized_data)\n",
    "\n",
    "# Normalize training data with leading zero handling\n",
    "x_train = normalize_sequences_train(train_data, leading_zeros)\n",
    "\n",
    "# Normalize validation and test data without leading zero handling\n",
    "x_validation = normalize_sequences(validation_data)\n",
    "x_test = normalize_sequences(test_data)\n",
    "\n",
    "# Print the shapes of normalized data\n",
    "print(f\"Normalized training data shape: {x_train.shape}\")\n",
    "print(f\"Normalized validation data shape: {x_validation.shape}\")\n",
    "print(f\"Normalized testing data shape: {x_test.shape}\")\n"
   ],
   "id": "2df0b7f1c987df27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check any `NaN`, `inf` or Anomalous Values in the normalized dataset",
   "id": "350a584cd75cad20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify problematic sequences\n",
    "for i, seq in enumerate(x_train):\n",
    "    if np.isnan(seq).any():\n",
    "        print(f\"NaN detected in sequence index {i}\")\n",
    "        print(seq)  # Inspect the sequence\n"
   ],
   "id": "ff9f7f428b89d314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Debug a single sequence normalization\n",
    "sequence = train_data.iloc[0:lookback_window].copy()\n",
    "base = sequence.iloc[0]  # Baseline\n",
    "print(\"Base values (used for normalization):\")\n",
    "print(base)\n",
    "\n",
    "# Check if any baseline value is NaN\n",
    "if base.isnull().any():\n",
    "    print(\"NaN detected in baseline!\")\n",
    "\n",
    "# Perform normalization\n",
    "normalized_sequence = (sequence / base) - 1\n",
    "print(\"Normalized sequence:\")\n",
    "print(normalized_sequence)\n"
   ],
   "id": "1ee3ff0d2f8da3b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to validate normalized data\n",
    "def validate_normalized_data(normalized_data):\n",
    "    # Check for NaN values\n",
    "    nan_count = np.isnan(normalized_data).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} NaN values in the normalized data.\")\n",
    "        normalized_data[np.isnan(normalized_data)] = 0  # Replace NaN with 0\n",
    "\n",
    "    # Check for Inf values\n",
    "    inf_count = np.isinf(normalized_data).sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"Warning: Found {inf_count} Inf values in the normalized data.\")\n",
    "        normalized_data[np.isinf(normalized_data)] = 0  # Replace Inf with 0\n",
    "\n",
    "    # Check for negative or unexpected values (optional, based on your use case)\n",
    "    if (normalized_data < -1).sum() > 0:\n",
    "        print(\"Warning: Found values less than -1. Review the normalization process.\")\n",
    "\n",
    "    print(\"Validation complete. Normalized data is clean.\")\n",
    "    return normalized_data\n"
   ],
   "id": "11aba72e78b7ea1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Validate the normalized datasets\n",
    "x_train = validate_normalized_data(x_train)\n",
    "x_validation = validate_normalized_data(x_validation)\n",
    "x_test = validate_normalized_data(x_test)\n"
   ],
   "id": "8f73727c428d82d1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
